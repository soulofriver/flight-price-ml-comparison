âœˆï¸ Flight Price Prediction â€” ML Pipeline (XGBoost vs Random Forest)
A clean and reproducible Machine Learning pipeline for predicting flight ticket prices using XGBoost and Random Forest.
This project benchmarks two powerful treeâ€‘based models on a real-world Kaggle dataset and includes full preprocessing, feature engineering, model evaluation, and visualization.
ğŸ“Œ Project Overview
This project builds an endâ€‘toâ€‘end ML workflow for airfare prediction.
It includes:

Data cleaning and preprocessing

Handling numerical & categorical features

Oneâ€‘Hot Encoding

Logâ€‘transformed target regression

Training XGBoost & Random Forest models

Model evaluation using MAE, RMSE, and RÂ²

Visual comparison of actual vs predicted prices

5â€‘fold crossâ€‘validation for reliable benchmarking

Dataset source:
https://www.kaggle.com/datasets/shubhambathwal/flight-price-prediction

ğŸ§  Models Used
XGBoost Regressor
500 estimators

Learning rate: 0.05

Max depth: 8

Subsample & colsample tuning

Wrapped with TransformedTargetRegressor for logâ€‘target regression

Random Forest Regressor
200 trees

Bootstrap aggregation

Also wrapped with logâ€‘target regression for fair comparison

ğŸ› ï¸ Tech Stack
Python

NumPy / Pandas

Scikitâ€‘Learn

XGBoost

Matplotlib

ColumnTransformer + Pipeline API

ğŸ“Š Evaluation Metrics
Each model is evaluated using:

MAE â€” Mean Absolute Error

RMSE â€” Root Mean Squared Error

RÂ² Score

5â€‘Fold Crossâ€‘Validation

A plot comparing the first 100 predictions is also included.

ğŸ“ Project Structure

â”œâ”€â”€ data/

â”‚   â””â”€â”€ Clean_Dataset.csv

â”œâ”€â”€ notebooks/

â”‚   â””â”€â”€ model_training.ipynb

â”œâ”€â”€ src/

â”‚   â”œâ”€â”€ preprocessing.py

â”‚   â”œâ”€â”€ train_xgb.py

â”‚   â”œâ”€â”€ train_rf.py

â”‚   â””â”€â”€ evaluate.py

â”œâ”€â”€ README.md

â””â”€â”€ requirements.txt

ğŸ“ˆ Results Summary
XGBoost generally performs better on RMSE and RÂ²

Random Forest provides stable baseline performance

Logâ€‘target regression significantly improves both models
